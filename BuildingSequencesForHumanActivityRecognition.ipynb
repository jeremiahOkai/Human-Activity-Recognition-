{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from collections import Counter\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reading datafiles. \n",
    "This function creates dataFrames for train, test and validation datasets used for training the machine learning model. The read_data function takes two variables, i.e, the name of the participant ('GOTOV05) and the location of the accelerometer data. The validation_data contains 2 participants selected at random, while the participant name passed as a variable above is used as the test subject. The rest are used in training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name, folder = '/data/gotov_data/geneActive/'):\n",
    "    data_to_select_random = []\n",
    "    for data in glob.glob(folder+'*csv'): \n",
    "        data = os.path.splitext(os.path.basename(data))[0]\n",
    "        if data in [name, 'GOTOV03','GOTOV16', 'GOTOV23','GOTOV04', \n",
    "                    'GOTOV02','GOTOV19', 'GOTOV12']:\n",
    "            continue\n",
    "        else:\n",
    "            data_to_select_random.append(data)\n",
    "            \n",
    "    validation_data = ['GOTOV30','GOTOV08', 'GOTOV33','GOTOV17', \n",
    "                        'GOTOV35','GOTOV31', 'GOTOV10','GOTOV21','GOTOV28', \n",
    "                        'GOTOV09','GOTOV18','GOTOV07','GOTOV20', 'GOTOV11', \n",
    "                        'GOTOV29', 'GOTOV27']\n",
    "    \n",
    "    validation_data = random.sample(validation_data, 2)\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_val = pd.DataFrame()\n",
    "    \n",
    "    for patient in data_to_select_random:\n",
    "        df = pd.read_csv(folder+patient+\".csv\", header = 0, index_col = None, \n",
    "                         low_memory=False).dropna(axis=0, how='any')\n",
    "        df['participant'] = patient\n",
    "        \n",
    "        if patient in validation_data:\n",
    "            print('loading val file', patient)\n",
    "            df_val = df_val.append(df)\n",
    "        else:\n",
    "            print('loading training file', patient)\n",
    "            df_train = df_train.append(df)\n",
    "            \n",
    "    df_test = pd.read_csv(folder+name+\".csv\", header = 0, index_col = None, \n",
    "                          low_memory=False).dropna(axis=0, how='any')\n",
    "    df_test['participant'] = name  \n",
    "    cols = ['ankle_x', 'ankle_y', 'ankle_z', 'wrist_x', 'wrist_y', 'wrist_z', \n",
    "            'chest_x', 'chest_y', 'chest_z','time', 'labels', 'participant']\n",
    "    df_train = df_train[col]\n",
    "    df_val = df_val[col]\n",
    "    df_test = df_test[col]\n",
    "    print('Done creating DataFrame for all files')\n",
    "\n",
    "    return df_train, df_val, df_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization of the datasets. \n",
    "Here we standardize all the values to have a zero mean and a standard deviation of 1 using the standard scaler from sckit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizing_data(Xtrain, Xval, Xtest): \n",
    "    print('Scaling data....')\n",
    "    col = ['time','labels', 'participant']\n",
    "    cols = ['ankle_x', 'ankle_y', 'ankle_z', 'wrist_x', 'wrist_y', 'wrist_z', \n",
    "            'chest_x', 'chest_y', 'chest_z','time', 'labels', 'participant']\n",
    "    \n",
    "    X_train = Xtrain.drop(col,  axis=1)\n",
    "    y_train = Xtrain[col].values\n",
    "\n",
    "    X_test = Xtest.drop(col,  axis=1)\n",
    "    y_test = Xtest[col].values\n",
    "    \n",
    "    X_val = Xval.drop(col,  axis=1)\n",
    "    y_val = Xval[col].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_train = X_train.astype('float32')\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    X_test = X_test.values\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_val = X_val.values\n",
    "    X_val = X_val.astype('float32')\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    X_train = pd.concat([X_train, y_train], axis=1)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_val = pd.DataFrame(X_val)\n",
    "    y_val = pd.DataFrame(y_val)\n",
    "    X_val = pd.concat([X_val, y_val], axis=1)\n",
    "    X_val.columns = cols\n",
    "    \n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_test = pd.concat([X_test, y_test], axis=1)\n",
    "    X_test.columns = cols\n",
    "\n",
    "    print('Done scaling data....')\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build final sequences\n",
    "The sequence_builder function accepts the dataFrame returned by the standardizing_data function above and transforms the data inputs into sequences required for training the machine learning model. The to_sequences function is called within the sequence_builder function. It accepts 4 args, ie, the sequence size, the input features (x_values), target variable (y_label) and delta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(SEQUENCE_SIZE, X_values, y_label, delta, deltamax = 1):\n",
    "    # Select maximum number of instances for the given sequence size\n",
    "    idx_max = int(X_values.shape[0]/SEQUENCE_SIZE)*SEQUENCE_SIZE\n",
    "    # Make sequences\n",
    "    X_values = X_values[:idx_max].reshape(-1, SEQUENCE_SIZE, X_values.shape[1])\n",
    "    \n",
    "    # Check if deltaT > deltamax\n",
    "    idx = np.array(delta[:idx_max]).reshape((-1,SEQUENCE_SIZE)).max(axis=1) < deltamax\n",
    "    X_values = X_values[idx,:,:]\n",
    "    \n",
    "    y_label = [y_label[0]]*X_values.shape[0]\n",
    "\n",
    "    return X_values, y_label\n",
    "\n",
    "def sequence_builder(df_f):\n",
    "    print('Building sequences.....')\n",
    "    seqX, seqY = [], []\n",
    "    df_f.index = pd.to_datetime(df_f.index, unit = \"ms\")\n",
    "    \n",
    "    for i in df_f['participant'].unique():\n",
    "#         print(i)\n",
    "        for j in df_f['labels'].unique():\n",
    "            X_values = df_f.query(\"participant == '\"+ i +\"' and labels == '\" + j + \"'\")\n",
    "            if X_values.shape[0] == 0:\n",
    "#                 print(\"Participant \"+ i +\" has no class '\" + j + \"'\")\n",
    "                continue\n",
    "            deltaT = X_values.index.to_series().diff().dt.seconds.values\n",
    "            deltaT[0] = 0\n",
    "            y_label = X_values['labels']\n",
    "            X_values = X_values.drop(['time', 'labels','participant'], axis=1).values\n",
    "            x, y = to_sequences(200, X_values, y_label, deltaT, deltamax = 1)\n",
    "            seqX.append(x)\n",
    "            seqY.append(y)\n",
    "    xv = np.vstack(seqX)\n",
    "    yv = [item for sublist in seqY for item in sublist]\n",
    "    print('Done building sequences....')\n",
    "    return xv, yv\n",
    "\n",
    "\n",
    "def encode_permute_data(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    print('Encoding data....')\n",
    "    \n",
    "    y_train = pd.Series(y_train)\n",
    "    y_val= pd.Series(y_val)\n",
    "    all_labels = pd.concat([y_train, y_val])\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(all_labels)\n",
    "    \n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_val = encoder.transform(y_val)\n",
    "    y_test = encoder.transform(y_test)\n",
    "    \n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "    y_val = np_utils.to_categorical(y_val)\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5,37):\n",
    "    if i in [12, 19, 23, 16]: \n",
    "        continue\n",
    "    else: \n",
    "        if len(str(i)) == 1:\n",
    "            name = 'GOTOV0'+str(i)\n",
    "        else:\n",
    "            name = 'GOTOV'+str(i)\n",
    "    print('Build sequence for test patient', name)\n",
    "    df_train, df_val, df_test = read_data(name)\n",
    "    Xtrain, Xval, Xtest = standardizing_data(df_train, df_val, df_test)\n",
    "    X_train, y_train = sequence_builder(Xtrain)\n",
    "    X_val, y_val = sequence_builder(Xval)\n",
    "    X_test, y_test = sequence_builder(Xtest)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, encoder = encode_permute_data(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    with open('/home/s1931628/'+folder+'/'+name+'.pkl', 'wb') as f: \n",
    "        pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test, encoder), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
